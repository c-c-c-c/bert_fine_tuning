{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#JSONとOAuth認証用のライブラリを使用\n",
    "import json\n",
    "from requests_oauthlib import OAuth1Session\n",
    "# import setting\n",
    "import tweepy\n",
    "import os\n",
    "import pandas as pd\n",
    "# twitter = OAuth1Session(setting.CONSUMER_KEY, setting.CONSUMER_SECRET, setting.ACCESS_TOKEN, setting.ACCESS_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.read_json('../tweet_extructor/twitterJSA_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_f, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./train_twitterJSA_data.csv\")\n",
    "test_df.to_csv(\"./test_twitterJSA_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, tokenization_bert\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "import random\n",
    "\n",
    "model = BertModel.from_pretrained('bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers')\n",
    "tokenizer_bert  = BertTokenizer(\"bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt\",\n",
    "                               do_lower_case=False, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torchtext\n",
    "# from bert import BertTokenizer\n",
    "# from bert import BertTokenizer\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub(\"<br/>\", \"\", text)\n",
    "    \n",
    "    for p in string.punctuation:\n",
    "        if (p==\".\")or (p==\",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text=text.replace(p, \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)\n",
    "    return ret\n",
    "\n",
    "max_length = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -c 2500 ../tweet_extructor/twitterJSA_data.json # 文字化けは気にしなくて良いはず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は一つのjsonデータを読み込む\n",
    "dataset = torchtext.data.TabularDataset.splits(\n",
    "    path='../tweet_extructor/',\n",
    "    train=\"train_twitterJSA_data.csv\",\n",
    "    test=\"test_twitterJSA_data.csv\",\n",
    "    format='csv', fields= {\"text\": (\"text\", TEXT), \"label\":(\"label\", LABEL)} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# なぜかsplitで返り値が一つしかかえらない(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, val_ds = dataset[0].split() # ので、この式を実行するとエラー仕方ないので、下を実行\n",
    "# train_ds= dataset[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenization_bert.load_vocab(vocab_file=\"./bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT用の単語辞書を辞書型変数に用意します\n",
    "# from bert import  load_vocab\n",
    "# vocab_bert, ids_to_tokens_bert = tokenization_bert.load_vocab(vocab_file=\"./bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt\")\n",
    "vocab_bert = tokenization_bert.load_vocab(vocab_file=\"./bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt\")\n",
    "\n",
    "# 1度適当にbuild_vocabでボキャブラリーを作成してから、BERTのボキャブラリーを上書きします\n",
    "TEXT.build_vocab(\"\", min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torchtext.data.Iterator(\n",
    "#     train_ds[0], batch_size=3, train=True)\n",
    "    dataset[0], batch_size=3, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここで結局エラー・・・"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認 検証データのデータセットで確認\n",
    "batch = next(iter(train_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dl:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTwitterSuzuki(nn.Module):\n",
    "    '''BERTモデルにTwitterのポジ・ネガ系５分類を判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForIMDb, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "\n",
    "        # headにポジネガ予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元、出力はポジ・ネガの2つ\n",
    "        self.cls = nn.Linear(in_features=768, out_features=5)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # 入力文章の1単語目[CLS]の特徴量を使用して、ポジ・ネガを分類します\n",
    "        vec_0 = encoded_layers[:, 0, :]\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
    "        out = self.cls(vec_0)\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
